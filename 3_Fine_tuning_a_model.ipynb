{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hukim1112/one-day-LLM/blob/main/3_Fine_tuning_a_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5cvv2hUiAtc"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTJgnYcaiAtc"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the following line:\n",
        "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LcAaezFgFQ7"
      },
      "source": [
        "# Pytorch를 사용한 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1xkweyxiAtd"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "#토크나이저 로드: 설정한 체크포인트를 기반으로 토크나이저를 로드합니다. 이 토크나이저는 BERT 모델에 맞게 텍스트를 토큰 단위로 변환해줍니다.\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "#데이터셋 토크나이즈: map 메서드를 이용하여 데이터셋의 각 샘플에 tokenize_function을 적용해 토크나이즈된 데이터셋을 생성합니다. batched=True로 설정하여 배치 단위로 처리합니다.\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qa-knSs_iAtd"
      },
      "outputs": [],
      "source": [
        "# 모델이 원시 텍스트를 입력으로 받지 않으므로 텍스트 열을 제거합니다:\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
        "\n",
        "# 모델이 인자를 'labels'로 명명된 상태로 기대하므로 'label' 열의 이름을 'labels'로 변경합니다:\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# 데이터셋의 형식을 리스트 대신 PyTorch 텐서를 반환하도록 설정합니다:\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "# 학습 데이터셋의 열 이름을 출력합니다:\n",
        "tokenized_datasets[\"train\"].column_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhLTZwtliAte"
      },
      "outputs": [],
      "source": [
        "[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48u5SfpMiAte"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9qIwy2riAte"
      },
      "outputs": [],
      "source": [
        "for batch in train_dataloader:\n",
        "    break\n",
        "{k: v.shape for k, v in batch.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqLDo_gLjOZw"
      },
      "source": [
        "from_pretrained 메서드를 사용하여 사전 학습된 모델을 불러옵니다. checkpoint는 이전에 설정한 모델의 체크포인트(예: \"bert-base-uncased\")를 가리킵니다.\n",
        "num_labels=2는 분류 작업에서 사용할 클래스의 수를 설정합니다. 예를 들어, 이 코드에서는 이진 분류 작업(두 개의 클래스)을 수행하기 위해 num_labels를 2로 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BHe92wdiAtf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tD0LvB2jf5T"
      },
      "source": [
        "outputs.loss는 모델의 손실 값을 나타냅니다. 손실 값은 모델의 예측이 실제 값과 얼마나 차이가 있는지를 나타내는 지표로, 모델을 훈련시키는 과정에서 최소화하려고 합니다.\n",
        "outputs.logits는 모델의 예측값(로짓)입니다. 로짓은 각 클래스에 대한 모델의 확신도를 나타내며, 보통 소프트맥스 함수를 통해 확률로 변환됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz5dcYg3iAtf"
      },
      "outputs": [],
      "source": [
        "outputs = model(**batch)\n",
        "print(outputs.loss, outputs.logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJaiXEJbjvt3"
      },
      "source": [
        "transformers 라이브러리에서 AdamW 옵티마이저를 임포트합니다.\n",
        "모델의 파라미터를 입력으로 받아 학습률을 5e-5로 설정한 AdamW 옵티마이저를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvmNo_9ciAtf"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2Kg7eCSjrr4"
      },
      "source": [
        "get_scheduler를 임포트하여 학습률 스케줄러를 설정합니다.\n",
        "num_epochs를 3으로 설정하고, 전체 학습 스텝 수를 계산합니다.\n",
        "선형 스케줄러를 생성하여 지정된 학습 스텝 동안 학습률을 조정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fthuX5B5iAtg"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "print(num_training_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEt13CD1iAtg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUgCbJ72j4eM"
      },
      "source": [
        "모델을 학습 모드로 전환한 후, 에포크 수만큼 반복합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3suvr_hQiAtg"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss #모델에 전달하여 손실을 계산\n",
        "        loss.backward() # 역전파를 수행\n",
        "\n",
        "        optimizer.step() #옵티마이저 업데이트\n",
        "        lr_scheduler.step() #학습률 스케줄러 업데이트\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cumo6g_ulyIy"
      },
      "source": [
        "모델을 평가 모드로 전환한 후, metric을 계산합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7eyw9YPiAth"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "model.eval()\n",
        "for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZkbFVOFlchw"
      },
      "source": [
        "accelerator 사용\n",
        "\n",
        "Accelerate 라이브러리는 Hugging Face에서 제공하는 라이브러리로, 모델 학습을 가속화하고 PyTorch 기반의 분산 학습을 쉽게 구현할 수 있도록 도와줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kf6Ejta1iAth"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
        "\n",
        "accelerator = Accelerator()\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
        "\n",
        "train_dl, eval_dl, model, optimizer = accelerator.prepare(\n",
        "    train_dataloader, eval_dataloader, model, optimizer\n",
        ")\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dl)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dl:\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnH7F3iqf3Fb"
      },
      "source": [
        "# Transformer Trainer API 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJYc4UF5pzas"
      },
      "source": [
        "다음으로 Trainer API를 활용해 [glue 데이터셋](https://huggingface.co/datasets/nyu-mll/glue)의 mrpc 데이터셋을 학습해봅시다!\n",
        "\n",
        "🤗 Transformers는 모델의 훈련을 최적화한 [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) 클래스를 제공하여, 직접 훈련 루프를 작성하지 않고도 쉽게 훈련을 시작할 수 있습니다. [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) API는 로깅, 그래디언트 축적, 혼합 정밀도 등 다양한 훈련 옵션과 기능을 지원합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GLUE (General Language Understanding Evaluation) 벤치마크는 여러 다른 NLP 작업을 포함한 데이터셋 모음으로, 이 중 mrpc는 문장 쌍이 서로 의미상으로 동일한지를 판단하는 작업"
      ],
      "metadata": {
        "id": "OXhSIUkirvtw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyi06o97e5rO"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLzl9DwFlhR2"
      },
      "source": [
        "### Training hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGdgw7yXlhR3"
      },
      "source": [
        "다음으로, 다양한 하이퍼파라미터와 여러 훈련 옵션을 활성화하는 플래그들을 포함하는 [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) 클래스를 생성합니다. 이 튜토리얼에서는 기본 훈련 [하이퍼파라미터](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)로 시작할 수 있지만, 최적의 설정을 찾기 위해 이를 실험해 보아도 좋습니다.\n",
        "\n",
        "훈련 과정에서 생성된 체크포인트를 저장할 위치를 지정하세요:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eXQWp_Me5rO"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLELrEovsWN-"
      },
      "outputs": [],
      "source": [
        "training_args"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 로드하고 예상 레이블 수를 지정합니다."
      ],
      "metadata": {
        "id": "wRzq2mnQrx4d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLRjekg0e5rO"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vwt-oT0de5rO"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2L36uMhe5rP"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FkOlWK5e5rP"
      },
      "outputs": [],
      "source": [
        "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "print(predictions.predictions.shape, predictions.label_ids.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwLm0tzKe5rP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "preds = np.argmax(predictions.predictions, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdH025FIlhR3"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer)는 훈련 중 자동으로 모델 성능을 평가하지 않습니다. 성능을 계산하고 보고할 함수를 [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer)에 전달해야 합니다. [🤗 Evaluate](https://huggingface.co/docs/evaluate/index) 라이브러리는 간단한 다양한 평가 함수를 제공하며, 이를 [evaluate.load](https://huggingface.co/docs/evaluate/main/en/package_reference/loading_methods#evaluate.load) 함수로 로드할 수 있습니다. 자세한 내용은 이 [quicktour](https://huggingface.co/docs/evaluate/a_quick_tour)를 참조하세요:"
      ],
      "metadata": {
        "id": "CgZlE2YQhXwc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1WbCVQze5rP"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "metric.compute(predictions=preds, references=predictions.label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MNQtO7pe5rP"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC_LuIsslhR3"
      },
      "source": [
        "### Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELD9_eOflhR3"
      },
      "source": [
        "다음으로, 모델, 훈련 인자, 훈련 및 테스트 데이터셋, 평가 함수를 포함하여 Trainer 객체를 생성합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFYL7tSKe5rQ"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBFea17te5rQ"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgMbYeATlYq1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}