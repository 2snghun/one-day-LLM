{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hukim1112/one-day-LLM/blob/main/6_Alpaca_finetunning_with_WandB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3",
      "metadata": {
        "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3"
      },
      "source": [
        "# From Llama to Alpaca: Finetunning and LLM with Weights & Biases\n",
        "\n",
        "이 Notebook에서는 사전 훈련된 LLama 모델을 인스트럭션 데이터셋에 대해 미세 조정(fine-tuning)하는 방법을 배울 것입니다. davinci-003 (GPT-3)으로 생성된 데이터 대신 GPT-4를 사용하여 더욱 향상된 인스트럭션 데이터셋을 활용하는 업데이트된 버전의 Alpaca 데이터셋을 사용합니다. 자세한 내용은 [공식 저장소 페이지](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data)를 참조하세요.\n",
        "\n",
        "이 Notebook은 최소 24GB 메모리를 갖춘 A100/A10 GPU가 필요합니다. 매개변수를 조정하여 T4에서 실행할 수도 있지만 실행 시간이 매우 길어집니다.\n",
        "\n",
        "이 Notebook에는 연관 프로젝트인 알파카에 대한 보고서: [wandb](wandb.me/alpaca)가 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03ef319f-bf26-4192-8951-8d536181ab67",
      "metadata": {
        "id": "03ef319f-bf26-4192-8951-8d536181ab67"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "!pip install git+https://github.com/huggingface/transformers@v4.31-release\n",
        "!pip install pip install accelerate -U"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7804f904-5746-4530-867d-c766f4501dea",
      "metadata": {
        "id": "7804f904-5746-4530-867d-c766f4501dea"
      },
      "source": [
        "## Prepare your Instruction Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59bd7517-70d9-4dee-9f92-1a2891caf385",
      "metadata": {
        "id": "59bd7517-70d9-4dee-9f92-1a2891caf385"
      },
      "source": [
        "인스트럭션 데이터셋은 사용자의 특정 도메인과 관련된 명령/결과 쌍의 목록입니다. 예를 들어, 특정 분야의 질문과 답변, 기술 분야의 문제와 해결책, 또는 단순히 명령과 결과가 될 수 있습니다. 일반적인 예로는 \"jsonL 파일을 읽고 처음 5줄을 출력하는 파이썬 스크립트를 작성하라\"가 있으며, 이때 모델은 다음과 유사한 내용을 출력할 수 있습니다:\n",
        "\n",
        "\n",
        "```python\n",
        "import json\n",
        "\n",
        "fname = \"my_file.json\"\n",
        "\n",
        "# read file from fname\n",
        "with open(fname, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(data[0:5])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da04c0a5-f481-4364-880d-10c254388987",
      "metadata": {
        "id": "da04c0a5-f481-4364-880d-10c254388987"
      },
      "source": [
        "the Alpaca (GPT-4 curated instructions and outputs) 데이터셋을 가져옵니다.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9",
      "metadata": {
        "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0665a80-6137-4a61-a3da-93bde606df04",
      "metadata": {
        "id": "b0665a80-6137-4a61-a3da-93bde606df04"
      },
      "source": [
        "데이터셋을 로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fce67d2-3703-4042-816c-7a13ba9eab3e",
      "metadata": {
        "id": "0fce67d2-3703-4042-816c-7a13ba9eab3e"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "dataset_file = \"alpaca_gpt4_data.json\"\n",
        "\n",
        "with open(dataset_file, \"r\") as f:\n",
        "    alpaca = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9618cd92-acdd-471b-9521-d55c38af8040",
      "metadata": {
        "id": "9618cd92-acdd-471b-9521-d55c38af8040"
      },
      "outputs": [],
      "source": [
        "type(alpaca), alpaca[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fXvc-WZKWcEU",
      "metadata": {
        "id": "fXvc-WZKWcEU"
      },
      "outputs": [],
      "source": [
        "len(alpaca)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e596e369-56aa-4721-9271-6686eed8fb35",
      "metadata": {
        "id": "e596e369-56aa-4721-9271-6686eed8fb35"
      },
      "source": [
        "데이터셋에는 명령(instruction)과 결과(output)가 포함되어 있습니다. 모델은 다음 토큰을 예측하도록 훈련되므로, 한 가지 방법은 단순히 둘을 연결(concatenate)하고 그 결과를 토대로 모델을 훈련하는 것입니다. 이상적으로 프롬프트는 입력과 출력 위치를 명확하게 표시하는 방식으로 구성되어야 합니다. 모든 것을 체계적이게 유지하기 위해 dataset을 W&B (Weights & Biases)에 기록해 보겠습니다.\n",
        "\n",
        "wandb는 \"Weights & Biases\"의 약자로, 머신 러닝 실험을 추적하고, 시각화하며, 공유하기 위한 툴입니다. 이 라이브러리는 머신러닝 프로젝트를 관리하기 위한 다양한 기능을 제공합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f98ce99-704b-469f-b490-04abe3bfc7c7",
      "metadata": {
        "id": "4f98ce99-704b-469f-b490-04abe3bfc7c7"
      },
      "source": [
        "### Train/Eval Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dfa1958-c58d-4b40-a7d9-5e0cc6d2abf5",
      "metadata": {
        "id": "5dfa1958-c58d-4b40-a7d9-5e0cc6d2abf5"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "seed = 42\n",
        "\n",
        "random.seed(seed)\n",
        "random.shuffle(alpaca)  # this could also be a parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5492465c-c1b8-4f04-a154-36ce3bcb6610",
      "metadata": {
        "id": "5492465c-c1b8-4f04-a154-36ce3bcb6610"
      },
      "outputs": [],
      "source": [
        "train_dataset = alpaca[:-1000]\n",
        "eval_dataset = alpaca[-1000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72fc9e43-55b5-4a7b-902f-b8a3b82dbf06",
      "metadata": {
        "id": "72fc9e43-55b5-4a7b-902f-b8a3b82dbf06"
      },
      "source": [
        "데이터셋을 학습(training) 데이터셋과 평가(evaluation) 데이터셋으로 분할하고, 이를 JSON 형식으로 저장한 후, Weights & Biases(W&B) 플랫폼에 로깅합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iQibuAgb69iA",
      "metadata": {
        "id": "iQibuAgb69iA"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdbc0fd0-de6c-447d-abb9-3176c6ceeaf6",
      "metadata": {
        "id": "fdbc0fd0-de6c-447d-abb9-3176c6ceeaf6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.DataFrame(train_dataset)\n",
        "eval_df = pd.DataFrame(eval_dataset)\n",
        "\n",
        "train_table = wandb.Table(dataframe=train_df)\n",
        "eval_table  = wandb.Table(dataframe=eval_df)\n",
        "\n",
        "train_df.to_json(\"alpaca_gpt4_train.jsonl\", orient='records', lines=True)\n",
        "eval_df.to_json(\"alpaca_gpt4_eval.jsonl\", orient='records', lines=True)\n",
        "\n",
        "with wandb.init(project=\"alpaca_ft\", job_type=\"split_data\"):\n",
        "    at = wandb.Artifact(\n",
        "        name=\"alpaca_gpt4_splitted\",\n",
        "        type=\"dataset\",\n",
        "        description=\"A GPT4 generated Alpaca like dataset for instruction finetunning\",\n",
        "        metadata={\"url\":\"https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data\"},\n",
        "    )\n",
        "    at.add_file(\"alpaca_gpt4_train.jsonl\")\n",
        "    at.add_file(\"alpaca_gpt4_eval.jsonl\")\n",
        "    wandb.log_artifact(at)\n",
        "    wandb.log({\"train_dataset\":train_table, \"eval_dataset\":eval_table})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26cb96e0-b2f2-4a79-ba65-e1c8d6395d54",
      "metadata": {
        "id": "26cb96e0-b2f2-4a79-ba65-e1c8d6395d54"
      },
      "source": [
        "데이터셋을 테이블로 로깅하고 이것을 워크스페이스에 검사할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ece48bbf-ddc0-4507-a733-83c5b3c1c20d",
      "metadata": {
        "id": "ece48bbf-ddc0-4507-a733-83c5b3c1c20d"
      },
      "outputs": [],
      "source": [
        "def prompt_no_input(row):\n",
        "    return (\"Below is an instruction that describes a task. \"\n",
        "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "            \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\").format_map(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "024aec96-40fb-4417-8e64-060a301b0f0b",
      "metadata": {
        "id": "024aec96-40fb-4417-8e64-060a301b0f0b"
      },
      "outputs": [],
      "source": [
        "row = alpaca[0]\n",
        "print(prompt_no_input(row))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e6f6a55-fd66-4215-bee7-1a05ef91e037",
      "metadata": {
        "id": "8e6f6a55-fd66-4215-bee7-1a05ef91e037"
      },
      "source": [
        "어떤 instruction은 input 변수 안에 context가 들어있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99e42d41-a95a-41de-a3cc-1461d9e10ed1",
      "metadata": {
        "id": "99e42d41-a95a-41de-a3cc-1461d9e10ed1"
      },
      "outputs": [],
      "source": [
        "row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b795343f-0356-4689-8bc6-9ac650716c8b",
      "metadata": {
        "id": "b795343f-0356-4689-8bc6-9ac650716c8b"
      },
      "outputs": [],
      "source": [
        "def prompt_input(row):\n",
        "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\").format_map(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04d0035e-96e6-4d69-ba1f-06e0051a3db6",
      "metadata": {
        "id": "04d0035e-96e6-4d69-ba1f-06e0051a3db6"
      },
      "outputs": [],
      "source": [
        "row = alpaca[9]\n",
        "print(prompt_input(row))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24f74b3e-8ca2-4c57-b37f-225164c2cb5a",
      "metadata": {
        "id": "24f74b3e-8ca2-4c57-b37f-225164c2cb5a"
      },
      "source": [
        "일단은 프롬프트를 처리합니다. 나중에 적절한 양의 패딩(padding)과 함께 결과를 추가할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cee98efa-c7ed-43a9-94bc-aad7e0da735f",
      "metadata": {
        "id": "cee98efa-c7ed-43a9-94bc-aad7e0da735f"
      },
      "source": [
        "And the refactored function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4038166-085c-4b10-a56a-2bee0bd62436",
      "metadata": {
        "id": "c4038166-085c-4b10-a56a-2bee0bd62436"
      },
      "outputs": [],
      "source": [
        "def create_alpaca_prompt(row):\n",
        "    return prompt_no_input(row) if row[\"input\"] == \"\" else prompt_input(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c34cc42-c38c-4e6f-bfb6-0f32d48aef5d",
      "metadata": {
        "id": "6c34cc42-c38c-4e6f-bfb6-0f32d48aef5d"
      },
      "source": [
        "## Why are we doing all this?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ee0cbc7-7d45-45b4-8e32-f3153b0d9ce2",
      "metadata": {
        "id": "7ee0cbc7-7d45-45b4-8e32-f3153b0d9ce2"
      },
      "source": [
        "우리가 업로드한 artifact에서 파일을 다시 로드할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb58a76e-c50c-4431-8584-3a4597c2a0a0",
      "metadata": {
        "id": "cb58a76e-c50c-4431-8584-3a4597c2a0a0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from wandb import Api\n",
        "\n",
        "api = Api()\n",
        "artifact = api.artifact('capecape/alpaca_ft/alpaca_gpt4_splitted:v4', type='dataset')\n",
        "dataset_dir = artifact.download()\n",
        "\n",
        "def load_jsonl(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "train_dataset = load_jsonl(f\"{dataset_dir}/alpaca_gpt4_train.jsonl\")\n",
        "eval_dataset = load_jsonl(f\"{dataset_dir}/alpaca_gpt4_eval.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c31dd88-e90e-4697-b354-b39eed0fab3c",
      "metadata": {
        "id": "8c31dd88-e90e-4697-b354-b39eed0fab3c"
      },
      "source": [
        "아주 특정한 방법으로 토큰화를 해야만 모델이 결과(output)를 예측하도록 학습할 수 있기 때문입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41703ce9-a22d-4245-9f6c-a424afd9ef11",
      "metadata": {
        "id": "41703ce9-a22d-4245-9f6c-a424afd9ef11"
      },
      "outputs": [],
      "source": [
        "train_prompts = [create_alpaca_prompt(row) for row in train_dataset]\n",
        "eval_prompts = [create_alpaca_prompt(row) for row in eval_dataset]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a4c16a0-989b-4e17-bc07-e1cc95971813",
      "metadata": {
        "id": "0a4c16a0-989b-4e17-bc07-e1cc95971813"
      },
      "outputs": [],
      "source": [
        "print(train_prompts[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc69153b-eb20-4f6d-afba-5b737d36320b",
      "metadata": {
        "id": "cc69153b-eb20-4f6d-afba-5b737d36320b"
      },
      "source": [
        "우리는 target을 처리하고 문자열 종료 토큰(EOS)을 추가해야 합니다. LLama의 경우 이는: `\"</s>\"` 입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06177a5c-84ff-4be3-8d2b-6a483c8ae5e4",
      "metadata": {
        "id": "06177a5c-84ff-4be3-8d2b-6a483c8ae5e4"
      },
      "outputs": [],
      "source": [
        "def pad_eos(ds):\n",
        "    EOS_TOKEN = \"</s>\"\n",
        "    return [f\"{row['output']}{EOS_TOKEN}\" for row in ds]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc5b21e8-3d5b-4964-be7b-faff5038f7f3",
      "metadata": {
        "id": "dc5b21e8-3d5b-4964-be7b-faff5038f7f3"
      },
      "outputs": [],
      "source": [
        "train_outputs = pad_eos(train_dataset)\n",
        "eval_outputs = pad_eos(eval_dataset)\n",
        "train_outputs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b42190f2-20cf-4960-866b-ccb7700b5b20",
      "metadata": {
        "id": "b42190f2-20cf-4960-866b-ccb7700b5b20"
      },
      "source": [
        "좋습니다! examples라는 변수에 최종 버전을 저장합시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdea7bc4-1ec3-451e-ab00-549aa2056800",
      "metadata": {
        "id": "cdea7bc4-1ec3-451e-ab00-549aa2056800"
      },
      "outputs": [],
      "source": [
        "train_dataset = [{\"prompt\":s, \"output\":t, \"example\": s + t} for s, t in zip(train_prompts, train_outputs)]\n",
        "eval_dataset = [{\"prompt\":s, \"output\":t, \"example\": s + t} for s, t in zip(eval_prompts, eval_outputs)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bad92d95-23d5-474c-9271-59948d5dcbb0",
      "metadata": {
        "id": "bad92d95-23d5-474c-9271-59948d5dcbb0"
      },
      "source": [
        "이것이 모델이 보고 배울 필요가 있는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e923b9bb-ced2-44f9-88f3-1c7690dde802",
      "metadata": {
        "id": "e923b9bb-ced2-44f9-88f3-1c7690dde802"
      },
      "outputs": [],
      "source": [
        "print(train_dataset[0][\"example\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5270873e-53d4-492b-bdce-b4d8ed8084bd",
      "metadata": {
        "id": "5270873e-53d4-492b-bdce-b4d8ed8084bd"
      },
      "source": [
        "## Converting text to numbers: Tokenizer\n",
        "\n",
        "우리는 데이터셋을 토큰들로 변환할 필요가 있습니다. 이것은 transformers 라이브러리로 쉽게 달성할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "720c707b-3bce-4164-b8c1-3c3122200c39",
      "metadata": {
        "id": "720c707b-3bce-4164-b8c1-3c3122200c39"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4162aec8-f2ba-45db-9633-817b416d4e57",
      "metadata": {
        "id": "4162aec8-f2ba-45db-9633-817b416d4e57"
      },
      "outputs": [],
      "source": [
        "#model_id = 'meta-llama/Llama-2-7b-hf\n",
        "model_id = 'NousResearch/Llama-2-7b-chat-hf'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "337fedfd-e238-4e86-a96b-24dfeed11f8a",
      "metadata": {
        "id": "337fedfd-e238-4e86-a96b-24dfeed11f8a"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"My experiments are going strong!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20c69466-f7e6-45b8-a167-718c80cedc0f",
      "metadata": {
        "id": "20c69466-f7e6-45b8-a167-718c80cedc0f"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"My experiments are going strong!\", padding='max_length', max_length=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb1e5e29-254a-4dbf-ac02-ea6bb2a16e77",
      "metadata": {
        "id": "cb1e5e29-254a-4dbf-ac02-ea6bb2a16e77"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"My experiments are going strong!\",\n",
        "                 padding='max_length',\n",
        "                 max_length=10,\n",
        "                 return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a89a61d-34b7-4a56-b1d8-98ebcf3384d7",
      "metadata": {
        "id": "0a89a61d-34b7-4a56-b1d8-98ebcf3384d7"
      },
      "outputs": [],
      "source": [
        "tokenizer([\"My experiments are going strong!\",\n",
        "           \"I love Llamas\"],\n",
        "          padding='max_length',\n",
        "          # padding='longest',\n",
        "          max_length=10,\n",
        "          return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58586813-7918-4578-9583-df14c5a6a6ad",
      "metadata": {
        "id": "58586813-7918-4578-9583-df14c5a6a6ad"
      },
      "source": [
        "### Packing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d316d169-4292-41aa-a42d-cd49620a881c",
      "metadata": {
        "id": "d316d169-4292-41aa-a42d-cd49620a881c"
      },
      "source": [
        "우리는 몇 개의 짧은 examples을 더 긴 chunk로 포장(packing)합니다. 이걸로 우리는 좀 더 효율적으로 학습할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af6df54e-f6bc-4e56-aec2-014a19fc654c",
      "metadata": {
        "id": "af6df54e-f6bc-4e56-aec2-014a19fc654c"
      },
      "source": [
        "여기서의 주요 아이디어는 지시사항/출력 샘플이 짧다는 것입니다. 그러니 EOS 토큰으로 구분지어 여러 개를 연결합시다. 데이터셋을 사전에 토큰화하고 사전에 패킹함으로써 모든 것을 더 빠르게 처리할 수 있습니다! 만약 max_seq_len = 1024로 설정한다면, 패킹하는 코드는 다음과 같이 보일 것입니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a537da60-db69-42a7-8c66-0ea3756c2847",
      "metadata": {
        "id": "a537da60-db69-42a7-8c66-0ea3756c2847"
      },
      "outputs": [],
      "source": [
        "max_sequence_len = 1024\n",
        "\n",
        "def pack(dataset, max_seq_len=max_sequence_len):\n",
        "    tkds_ids = tokenizer([s[\"example\"] for s in dataset])[\"input_ids\"]\n",
        "\n",
        "    all_token_ids = []\n",
        "    for tokenized_input in tkds_ids:\n",
        "        all_token_ids.extend(tokenized_input)# + [tokenizer.eos_token_id])\n",
        "\n",
        "    print(f\"Total number of tokens: {len(all_token_ids)}\")\n",
        "    packed_ds = []\n",
        "    for i in range(0, len(all_token_ids), max_seq_len+1):\n",
        "        input_ids = all_token_ids[i : i + max_seq_len+1]\n",
        "        if len(input_ids) == (max_seq_len+1):\n",
        "            packed_ds.append({\"input_ids\": input_ids[:-1], \"labels\": input_ids[1:]})  # this shift is not needed if using the model.loss\n",
        "    return packed_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c78b4e9-2c7f-4aa1-b738-be04bc55b06b",
      "metadata": {
        "id": "9c78b4e9-2c7f-4aa1-b738-be04bc55b06b"
      },
      "outputs": [],
      "source": [
        "train_ds_packed = pack(train_dataset)\n",
        "eval_ds_packed = pack(eval_dataset)\n",
        "len(train_ds_packed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pbPIYAop0xRo",
      "metadata": {
        "id": "pbPIYAop0xRo"
      },
      "outputs": [],
      "source": [
        "eval_ds_packed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df389230-911c-447c-a177-a18c22837020",
      "metadata": {
        "id": "df389230-911c-447c-a177-a18c22837020"
      },
      "source": [
        "이렇게 하면, 길이가 1024인 11,000개 이상의 시퀀스를 얻게 됩니다.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d43713b3-9f65-42a6-8338-ab0601e5f476",
      "metadata": {
        "id": "d43713b3-9f65-42a6-8338-ab0601e5f476"
      },
      "source": [
        "### DataLoader\n",
        "일반적인 크로스 엔트로피로 훈련을 하고 이 패킹된 데이터셋에서 다음 토큰을 예측할 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f342873a-29a3-4ed1-8b59-9e7f7f2a4c1d",
      "metadata": {
        "id": "f342873a-29a3-4ed1-8b59-9e7f7f2a4c1d"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import default_data_collator\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "batch_size = 16  # I have an A100 GPU with 40GB of RAM 😎\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_ds_packed,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=default_data_collator, # we don't need any special collator 😎\n",
        ")\n",
        "\n",
        "eval_dataloader = DataLoader(\n",
        "    eval_ds_packed,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=default_data_collator,\n",
        "    shuffle=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be2ccaf4-dfa1-4daa-9a6f-244c8cda7818",
      "metadata": {
        "id": "be2ccaf4-dfa1-4daa-9a6f-244c8cda7818"
      },
      "source": [
        "batch가 어떻게 생겼는지를 확인합니다. 데이터로더로부터 다음과 같이 샘플링할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffced7ec-bd1b-42b0-be64-04f3fae5df84",
      "metadata": {
        "id": "ffced7ec-bd1b-42b0-be64-04f3fae5df84"
      },
      "outputs": [],
      "source": [
        "b = next(iter(train_dataloader))\n",
        "b"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33d15d30-58d2-465d-a9f4-be0eef2ea06b",
      "metadata": {
        "id": "33d15d30-58d2-465d-a9f4-be0eef2ea06b"
      },
      "source": [
        "We can alos decode the batch just to be super sure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28e18ad2-c141-4902-a5a4-24a1e743be35",
      "metadata": {
        "id": "28e18ad2-c141-4902-a5a4-24a1e743be35"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(b[\"input_ids\"][0])[:250]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0a46e93-7ec2-4365-8e50-be7bef873436",
      "metadata": {
        "id": "f0a46e93-7ec2-4365-8e50-be7bef873436"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(b[\"labels\"][0])[:250]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb99a7dc-0654-4985-ac3f-60ecdc6f6558",
      "metadata": {
        "id": "fb99a7dc-0654-4985-ac3f-60ecdc6f6558"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9757e458-14fd-4dd3-861f-efad04dce787",
      "metadata": {
        "id": "9757e458-14fd-4dd3-861f-efad04dce787"
      },
      "source": [
        "다음과 같이 모든 하이퍼파라미터들을 관리합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6925a62e-d85e-4c86-8867-bee3a180fc08",
      "metadata": {
        "id": "6925a62e-d85e-4c86-8867-bee3a180fc08"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "gradient_accumulation_steps = 2\n",
        "\n",
        "config = SimpleNamespace(\n",
        "    model_id=model_id,\n",
        "    dataset_name=\"alpaca-gpt4\",\n",
        "    precision=\"bf16\",  # faster and better than fp16, requires new GPUs\n",
        "    n_freeze=24,  # How many layers we don't train, LLama 7B has 32.\n",
        "    lr=2e-4,\n",
        "    n_eval_samples=10, # How many samples to generate on validation\n",
        "    max_seq_len=max_sequence_len, # Lenght of the sequences to pack\n",
        "    epochs=3,  # we do 3 pasess over the dataset.\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,  # evey how many iterations we update the gradients, simulates larger batch sizes\n",
        "    batch_size=batch_size,  # what my GPU can handle, depends on how many layers are we training\n",
        "    log_model=False,  # upload the model to W&B?\n",
        "    gradient_checkpointing = True,  # saves even more memory\n",
        "    freeze_embed = True,  # why train this? let's keep them frozen ❄️\n",
        "    seed=seed,\n",
        ")\n",
        "\n",
        "config.total_train_steps = config.epochs * len(train_dataloader) // config.gradient_accumulation_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a7166a6-8e88-4f0d-bc0e-70aac292c645",
      "metadata": {
        "id": "0a7166a6-8e88-4f0d-bc0e-70aac292c645"
      },
      "outputs": [],
      "source": [
        "print(f\"We will train for {config.total_train_steps} steps and evaluate every epoch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bcd0229-5e4c-4bb5-827b-309bdbb351df",
      "metadata": {
        "id": "7bcd0229-5e4c-4bb5-827b-309bdbb351df"
      },
      "source": [
        "pretrained model을 가져옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51c10f7f-2551-4aa4-aef2-29c888b57a12",
      "metadata": {
        "id": "51c10f7f-2551-4aa4-aef2-29c888b57a12"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.model_id,\n",
        "    device_map=0,\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    use_cache=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc6c3959-854c-409e-9037-b5c87a6adce5",
      "metadata": {
        "id": "fc6c3959-854c-409e-9037-b5c87a6adce5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def param_count(m):\n",
        "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
        "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
        "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
        "    return params, trainable_params\n",
        "\n",
        "params, trainable_params = param_count(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f79dbbb0-6dac-4f78-a862-8201088c9d57",
      "metadata": {
        "id": "f79dbbb0-6dac-4f78-a862-8201088c9d57"
      },
      "source": [
        "전체 모델을 학습하는 것은 강력한 연산력과 메모리를 필요로하기 때문에 우리는 8개의 layer를 튜닝할 것 입니다. LLama는 총 32개를 가지고 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c4de41e-5c10-478b-9524-f4a3119d277c",
      "metadata": {
        "id": "4c4de41e-5c10-478b-9524-f4a3119d277c"
      },
      "outputs": [],
      "source": [
        "# freeze layers (disable gradients)\n",
        "for param in model.parameters(): param.requires_grad = False\n",
        "for param in model.lm_head.parameters(): param.requires_grad = True\n",
        "for param in model.model.layers[config.n_freeze:].parameters(): param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd7fc941-65dc-4dee-839d-351bd019a2fb",
      "metadata": {
        "id": "fd7fc941-65dc-4dee-839d-351bd019a2fb"
      },
      "outputs": [],
      "source": [
        "# Just freeze embeddings for small memory decrease\n",
        "if config.freeze_embed:\n",
        "    model.model.embed_tokens.weight.requires_grad_(False);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "284599f6-ba88-4172-a311-8e618f716b30",
      "metadata": {
        "id": "284599f6-ba88-4172-a311-8e618f716b30"
      },
      "source": [
        "또한 그래디언트 체크포인팅을 사용하여 더 많이 저장할 수도 있습니다(이것은 훈련을 느리게 만들지만, 얼마나 느려질지는 여러분의 특정 설정에 따라 달라집니다). 대용량 모델을 메모리에 맞추는 방법에 대해 허깅페이스 웹사이트에 [좋은 아티클](https://huggingface.co/docs/transformers/v4.18.0/en/performance)이 있으니 확인해 보시길 권장합니다!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "750ce64e-0088-4ca8-9bc9-60037e7110d3",
      "metadata": {
        "id": "750ce64e-0088-4ca8-9bc9-60037e7110d3"
      },
      "outputs": [],
      "source": [
        "# save more memory\n",
        "if config.gradient_checkpointing:\n",
        "    model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6232ce7a-b847-45c8-8ff7-e36249e7a060",
      "metadata": {
        "id": "6232ce7a-b847-45c8-8ff7-e36249e7a060"
      },
      "outputs": [],
      "source": [
        "params, trainable_params = param_count(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc92663c-95a4-4ecc-a9af-7a68d9648271",
      "metadata": {
        "id": "dc92663c-95a4-4ecc-a9af-7a68d9648271"
      },
      "source": [
        "### Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5374c44d-517b-42a0-ade6-297c0a5d18b3",
      "metadata": {
        "id": "5374c44d-517b-42a0-ade6-297c0a5d18b3"
      },
      "outputs": [],
      "source": [
        "from transformers import get_cosine_schedule_with_warmup\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(), lr=config.lr, betas=(0.9,0.99), eps=1e-5)\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optim,\n",
        "    num_training_steps=config.total_train_steps,\n",
        "    num_warmup_steps=config.total_train_steps // 10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5efdd402-c851-47a5-9134-5b47b7d118e7",
      "metadata": {
        "id": "5efdd402-c851-47a5-9134-5b47b7d118e7"
      },
      "outputs": [],
      "source": [
        "def loss_fn(x, y):\n",
        "    \"A Flat CrossEntropy\"\n",
        "    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24440392-9837-4cbb-873a-372f9f5aca20",
      "metadata": {
        "id": "24440392-9837-4cbb-873a-372f9f5aca20"
      },
      "source": [
        "## Testing during training\n",
        "\n",
        "거의 다 왔습니다, 이제 모델에서 샘플링하는 간단한 함수를 만들어 가끔 모델이 출력하는 것을 시각적으로 확인해 봅시다! 간단하게 모델.generate 메소드를 감싸 보겠습니다. GenerationConfig에서 기본 샘플링 매개변수를 가져와 해당 모델 ID를 전달하면 됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a92fe7-3f9e-43e6-80b0-adbbd8b480ee",
      "metadata": {
        "id": "e0a92fe7-3f9e-43e6-80b0-adbbd8b480ee"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "gen_config = GenerationConfig.from_pretrained(config.model_id)\n",
        "test_config = SimpleNamespace(\n",
        "    max_new_tokens=256,\n",
        "    gen_config=gen_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ec41718-52c6-4335-a534-2790d03ba069",
      "metadata": {
        "id": "9ec41718-52c6-4335-a534-2790d03ba069"
      },
      "outputs": [],
      "source": [
        "def generate(prompt, max_new_tokens=test_config.max_new_tokens, gen_config=gen_config):\n",
        "    tokenized_prompt = tokenizer(prompt, return_tensors='pt')['input_ids'].cuda()\n",
        "    with torch.inference_mode():\n",
        "        output = model.generate(tokenized_prompt,\n",
        "                            max_new_tokens=max_new_tokens,\n",
        "                            generation_config=gen_config)\n",
        "    return tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44601edc-db5b-40ea-bb74-9591ea4e7e50",
      "metadata": {
        "id": "44601edc-db5b-40ea-bb74-9591ea4e7e50"
      },
      "source": [
        "LoL 🤷"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4e39c49-ccb1-4fe6-aa30-988ea5583b99",
      "metadata": {
        "id": "b4e39c49-ccb1-4fe6-aa30-988ea5583b99"
      },
      "outputs": [],
      "source": [
        "prompt = eval_dataset[14][\"prompt\"]\n",
        "print(prompt + generate(prompt, 128))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "567920c0-005d-419d-98ab-4d797b243300",
      "metadata": {
        "id": "567920c0-005d-419d-98ab-4d797b243300"
      },
      "source": [
        "우리는 그 결과를 n 단계마다 프로젝트에 테이블로 기록할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bac25c48-cb18-4560-b05c-1748e7d1adf5",
      "metadata": {
        "id": "bac25c48-cb18-4560-b05c-1748e7d1adf5"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def prompt_table(examples, log=False, table_name=\"predictions\"):\n",
        "    table = wandb.Table(columns=[\"prompt\", \"generation\", \"concat\", \"output\", \"max_new_tokens\", \"temperature\", \"top_p\"])\n",
        "    for example in tqdm(examples, leave=False):\n",
        "        prompt, gpt4_output = example[\"prompt\"], example[\"output\"]\n",
        "        out = generate(prompt, test_config.max_new_tokens, test_config.gen_config)\n",
        "        table.add_data(prompt, out, prompt+out, gpt4_output, test_config.max_new_tokens, test_config.gen_config.temperature, test_config.gen_config.top_p)\n",
        "    if log:\n",
        "        wandb.log({table_name:table})\n",
        "    return table\n",
        "\n",
        "def to_gpu(tensor_dict):\n",
        "    return {k: v.to('cuda') for k, v in tensor_dict.items()}\n",
        "\n",
        "class Accuracy:\n",
        "    \"A simple Accuracy function compatible with HF models\"\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "        self.tp = 0.\n",
        "    def update(self, logits, labels):\n",
        "        logits, labels = logits.argmax(dim=-1).view(-1).cpu(), labels.view(-1).cpu()\n",
        "        tp = (logits == labels).sum()\n",
        "        self.count += len(logits)\n",
        "        self.tp += tp\n",
        "        return tp / len(logits)\n",
        "    def compute(self):\n",
        "        return self.tp / self.count"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a52fbc09-5d65-4265-abce-adda01d85584",
      "metadata": {
        "id": "a52fbc09-5d65-4265-abce-adda01d85584"
      },
      "source": [
        "원하신다면 검증을 빠르게 추가할 수도 있습니다. 이 단계에서 테이블을 생성할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6891b2c0-f22e-4647-9ac2-e07a994f3e96",
      "metadata": {
        "id": "6891b2c0-f22e-4647-9ac2-e07a994f3e96"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validate():\n",
        "    model.eval();\n",
        "    eval_acc = Accuracy()\n",
        "    loss, total_steps = 0., 0\n",
        "    for step, batch in enumerate(pbar:=tqdm(eval_dataloader, leave=False)):\n",
        "        pbar.set_description(f\"doing validation\")\n",
        "        batch = to_gpu(batch)\n",
        "        total_steps += 1\n",
        "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "            out = model(**batch)\n",
        "            loss += loss_fn(out.logits, batch[\"labels\"])  # you could use out.loss and not shift the dataset\n",
        "        eval_acc.update(out.logits, batch[\"labels\"])\n",
        "    # we log results at the end\n",
        "    wandb.log({\"eval/loss\": loss.item() / total_steps,\n",
        "               \"eval/accuracy\": eval_acc.compute()})\n",
        "    prompt_table(eval_dataset[:config.n_eval_samples], log=True)\n",
        "    model.train();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37364791-987e-4e81-9621-3094ed5bf86d",
      "metadata": {
        "id": "37364791-987e-4e81-9621-3094ed5bf86d"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "def save_model(model, model_name, models_folder=\"models\", log=False):\n",
        "    \"\"\"Save the model to wandb as an artifact\n",
        "    Args:\n",
        "        model (nn.Module): Model to save.\n",
        "        model_name (str): Name of the model.\n",
        "        models_folder (str, optional): Folder to save the model. Defaults to \"models\".\n",
        "    \"\"\"\n",
        "    model_name = f\"{wandb.run.id}_{model_name}\"\n",
        "    file_name = Path(f\"{models_folder}/{model_name}\")\n",
        "    file_name.parent.mkdir(parents=True, exist_ok=True)\n",
        "    model.save_pretrained(file_name, safe_serialization=True)\n",
        "    # save tokenizer for easy inference\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model.name_or_path)\n",
        "    tokenizer.save_pretrained(model_name)\n",
        "    if log:\n",
        "        at = wandb.Artifact(model_name, type=\"model\")\n",
        "        at.add_dir(file_name)\n",
        "        wandb.log_artifact(at)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93834c0b-15e1-4e43-8535-dbb9f36af29d",
      "metadata": {
        "id": "93834c0b-15e1-4e43-8535-dbb9f36af29d"
      },
      "source": [
        "모델 평가와 모델 출력을 table에 기록하는 루프를 정의합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e90177c-0c5a-48e7-9a39-2b9e67794814",
      "metadata": {
        "id": "6e90177c-0c5a-48e7-9a39-2b9e67794814"
      },
      "source": [
        "## The actual Loop\n",
        "- 그래디언트 누적 및 그래디언트 스케일링\n",
        "- 샘플링 및 모델 체크포인트 저장 (이것은 매우 빠르게 훈련되므로 여러 체크포인트를 저장할 필요가 없습니다)\n",
        "- 우리는 토큰 정확도를 계산합니다, 손실보다 더 나은 지표입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe75078a-5d35-46bc-8f33-0e3adf52d072",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fe75078a-5d35-46bc-8f33-0e3adf52d072"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=\"alpaca_ft\", # the project I am working on\n",
        "           tags=[\"baseline\",\"7b\"],\n",
        "           job_type=\"train\",\n",
        "           config=config) # the Hyperparameters I want to keep track of\n",
        "\n",
        "# Training\n",
        "acc = Accuracy()\n",
        "model.train()\n",
        "train_step = 0\n",
        "for epoch in tqdm(range(config.epochs)):\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "        batch = to_gpu(batch)\n",
        "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "            out = model(**batch)\n",
        "            loss = loss_fn(out.logits, batch[\"labels\"]) / config.gradient_accumulation_steps  # you could use out.loss and not shift the dataset\n",
        "            loss.backward()\n",
        "        if step%config.gradient_accumulation_steps == 0:\n",
        "            # we can log the metrics to W&B\n",
        "            wandb.log({\"train/loss\": loss.item() * config.gradient_accumulation_steps,\n",
        "                       \"train/accuracy\": acc.update(out.logits, batch[\"labels\"]),\n",
        "                       \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
        "                       \"train/global_step\": train_step})\n",
        "            optim.step()\n",
        "            scheduler.step()\n",
        "            optim.zero_grad(set_to_none=True)\n",
        "            train_step += 1\n",
        "    validate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4099bc28-e695-46a6-994c-b612f7811937",
      "metadata": {
        "id": "4099bc28-e695-46a6-994c-b612f7811937"
      },
      "outputs": [],
      "source": [
        "# we save the model checkpoint at the end\n",
        "#config.do_sample = True  # 샘플링을 활성화합니다.\n",
        "\n",
        "# del config.temperature  # temperature 설정을 제거합니다.\n",
        "# del config.top_p  # top_p 설정을 제거합니다.\n",
        "save_model(model, model_name=config.model_id.replace(\"/\", \"_\"), models_folder=\"models/\", log=config.log_model)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cec95b95-3e81-4a40-8eea-34048c992ba9",
      "metadata": {
        "id": "cec95b95-3e81-4a40-8eea-34048c992ba9"
      },
      "source": [
        "A100에서 약 70분 정도 소요됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a936e007-47fa-400f-873c-5e038bd685c5",
      "metadata": {
        "id": "a936e007-47fa-400f-873c-5e038bd685c5"
      },
      "source": [
        "## Full Eval Dataset evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "399fafb9-b41f-401b-a1c4-37e1ede2e639",
      "metadata": {
        "id": "399fafb9-b41f-401b-a1c4-37e1ede2e639"
      },
      "source": [
        "평가 데이터셋(eval_dataset)에서 모델 예측을 로그하는 테이블을 만들어 보겠습니다 (처음 250개 샘플에 대해서)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b89717a-ac70-43e8-9b7c-edd8d2c54cdc",
      "metadata": {
        "id": "3b89717a-ac70-43e8-9b7c-edd8d2c54cdc"
      },
      "outputs": [],
      "source": [
        "with wandb.init(project=\"alpaca_ft\", # the project I am working on\n",
        "           job_type=\"eval\",\n",
        "           config=config): # the Hyperparameters I want to keep track of\n",
        "    model.eval();\n",
        "    prompt_table(eval_dataset[:250], log=True, table_name=\"eval_predictions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jCh-S5b1dg00",
      "metadata": {
        "id": "jCh-S5b1dg00"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}